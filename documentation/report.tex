\documentclass[11pt, a4]{article}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsmath}
%\usepackage{tfrupee}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{multirow}
%\usepackage{fancyhdr}
%\usepackage{lastpage}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancyvrb}
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}%
{fontsize=\footnotesize,
	%
	frame=lines,  % top and bottom rule only
	framesep=2em, % separation between frame and text
	rulecolor=\color{Gray},
	%
	label=\fbox{\color{Black}requirements.txt},
	labelposition=topline,
	%
	commandchars=\|\(\), % escape character and argument delimiters for
	% commands within the verbatim
	commentchar=*        % comment character
}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}


%\topmargin -0.1in
% \textheight 11in
%\oddsidemargin -.01in
%\evensidemargin .0in
%\textwidth 6.5in
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstdefinelanguage{yaml}{
	morekeywords={true,false,null,y,n},
	sensitive=false,
	morecomment=[l]{\#},
	morestring=[b]',
	morestring=[b]",
	basicstyle=\ttfamily\footnotesize,
	keywordstyle=\color{magenta},
	commentstyle=\color{codegreen},
	stringstyle=\color{codepurple},
	literate=*{:}{{\textcolor{red}{:}}}{1},
}


\lstset{style=mystyle}
\begin{document}

	\author{Ritabrata Mandal\\ EE24E009}
	\title{DA6400 : Reinforcement Learning\\ Programming Assignment \#1\\ Report}
	\maketitle
	\medskip
	\newpage
	\tableofcontents
	\newpage
	\section{Introduction}
		\subsection{Environments}
		In this programming task, we are utilize the following \href{https://gymnasium.farama.org/}{\textcolor{magenta}{Gymnasium environments}} for training
		and evaluating your policies. The links associated with the environments contain descriptions
		of each environment.
		\begin{itemize}
			\item \href{https://gymnasium.farama.org/environments/classic_control/cart_pole/}{\textcolor{magenta}{CartPole-v1}} : A pole is attached by an un-actuated joint to a cart, which moves along	a frictionless track. The pendulum is placed upright on the cart and the goal is to
			balance the pole by applying forces in the left and right direction on the cart.
			\item \href{https://gymnasium.farama.org/environments/classic_control/mountain_car/}{\textcolor{magenta}{MountainCar-v0}} : The Mountain Car MDP is a deterministic MDP that consists of a
			car placed stochastically at the bottom of a sinusoidal valley, with the only possible
			actions being the accelerations that can be applied to the car in either direction. The
			goal of the MDP is to strategically accelerate the car to reach the goal state on top of
			the right hill. There are two versions of the mountain car domain in gymnasium: one
			with \textit{discrete actions} and one with \textit{continuous}. This version is the one with discrete
			actions.
			\item \href{https://minigrid.farama.org/environments/minigrid/DynamicObstaclesEnv/}{\textcolor{magenta}{MiniGrid-Dynamic-Obstacles-5x5-v0}} : This environment is an empty
			room with moving obstacles. The goal of the agent is to reach the green goal square
			without colliding with any obstacle. A large penalty is subtracted if the agent collides
			with an obstacle and the episode finishes. This environment is useful to test Dynamic
			Obstacle Avoidance for mobile robots with Reinforcement Learning in Partial Observability.
		\end{itemize}
		\subsection{Algorithms}
		Training each of the below algorithms and assessing their comparative performance.
		\begin{itemize}
			\item \textbf{SARSA}$\rightarrow$ with $\epsilon$-\textbf{greedy exploration}
			\item \textbf{Q-Learning}$\rightarrow$ with \textbf{Softmax exploration}
		\end{itemize}
	\section{Implementation}
		\subsection{\href{https://github.com/RitabrataMandal/RL-DA6400-assignment_1/tree/main/cartpole-v1}{\textcolor{magenta}{CartPole-v1}}}
			\subsubsection{Code Snippets}
				\lstinputlisting[language=Python,firstline=3, lastline=14, caption=Q-table  ]{../cartpole-v1/q_table.py}
				\lstinputlisting[language=Python,firstline=17, lastline=24, caption=Discretized sates ]{../cartpole-v1/q_table.py}
				\lstinputlisting[language=Python,firstline=3, lastline=15, caption=epsilon-greedy action selection]{../cartpole-v1/policy.py}
				\lstinputlisting[language=Python,firstline=5, lastline=50, caption= SARSA implementation ]{../cartpole-v1/sarsa_agent.py}
				\lstinputlisting[language=Python,firstline=17, lastline=29, caption=Softmax action selection ]{../cartpole-v1/policy.py}
				\lstinputlisting[language=Python,firstline=5, lastline=52, caption=Q-Learning implementation]{../cartpole-v1/q_agent.py}
%				\lstinputlisting[language=Python, caption=SARSA - Agent]{../cartpole-v1/sarsa_agent.py}
			\subsubsection{SARSA Hyper-Parameter Tuning}  
			Using Weights \& Biases (wandb) with a sweep method based on Bayesian optimization, we identified the best-performing hyper-parameters. Specifically, we set $\alpha \in [0.1, 0.5]$ and $\epsilon \in [0.01, 0.15]$, and ran 4000 episodes while minimizing the regret, defined as \(195 -\) (all-time average return). See the wandb report on this environment \href{https://api.wandb.ai/links/ee24e009-iitm/0hseqrps}{here}. Additionally, Figure \ref{fig:sarsacartpole-regret} displays the results from 50 sweeps, illustrating the relationship between $\alpha$, $\epsilon$, and the regret.
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\linewidth]{sarsa-hyp-tuning-cartpole.png}
					\caption{SARSA hyper-parameter sweeps for $\alpha$ and $\epsilon$ in CartPole-v1(4000 episodes), with lines color-coded by regret.}
					\label{fig:sarsacartpole-regret}
				\end{figure}
			\subsubsection{SARSA best $3$ results}
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|}
						\hline
						& $\alpha$ & $\epsilon$ & $\gamma$ & regret\\
						\hline
						1 & 0.26492  & 0.13824 & 0.99 & 65.7379\\
						\hline
						2 & 0.29568 & 0.12786 & 0.99 & 67.5396\\
						\hline
						3 & 0.24299 & 0.14852 & 0.99 & 68.09645\\
						\hline
					\end{tabular}					
				\end{center}
				Figure \ref{fig:sarsacartpole} include the episode vs reward plot for best three hyper-parameter combination.
				\begin{figure}[H]
					\centering
					\begin{subfigure}[h]{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../cartpole-v1/plots/sarsa_Figure_1.png}
						\caption{$1$ vs $2$}
						\label{fig:sarsacartpole1vs2}
					\end{subfigure}
					\hfill
					\begin{subfigure}[h]{0.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../cartpole-v1/plots/sarsa_Figure_2.png}
						\caption{$1$ vs $3$}
						\label{fig:sarsacartpole1vs3}
					\end{subfigure}
					\hfill
					\begin{subfigure}[h]{0.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../cartpole-v1/plots/sarsa_Figure_3.png}
						\caption{$2$ vs $3$}
						\label{fig:sarsacartpole2vs3}
					\end{subfigure}
					\caption{Performance comparison of the three best SARSA parameter configurations: (a) 1 vs 2, (b) 1 vs 3, and (c) 2 vs 3.}
					\label{fig:sarsacartpole}
				\end{figure}
			\subsubsection{Q-Learning hyper-parameter tuning}
				Using Weights \& Biases (wandb) with a sweep method based on Bayesian optimization, we identified the best-performing hyper-parameters. Specifically, we set $\alpha \in [0.1, 0.5]$ and $\tau \in [0.1, 1.5]$, and ran 4000 episodes while minimizing the regret, defined as \(195 -\) (all-time average return). See the wandb report on this environment \href{https://api.wandb.ai/links/ee24e009-iitm/il2a2cjz}{here}. Additionally, Figure \ref{fig:qlearningcartpole-regret} displays the results from 50 sweeps, illustrating the relationship between $\alpha$, $\tau$, and the regret.
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\linewidth]{qlearning-hyp-tuning-cartpole.png}
					\caption{Q-Learning hyper-parameter sweeps for $\alpha$ and $\tau$ in Cartpole-v1(4000 episodes), with lines color-coded by regret.}
					\label{fig:qlearningcartpole-regret}
				\end{figure}
			\subsubsection{Q-Learning best $3$ results}
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|}
						\hline
						& $\alpha$ & $\tau$ &$\gamma$ &regret\\
						\hline
						1 & 0.20034  & 1.11949  & 0.99 &76.7096\\
						\hline
						2 & 0.15243 & 1.02074 & 0.9 & 86.03815\\
						\hline
						3 & 0.15404 & 1.0012 & 0.9 & 92.8918\\
						\hline
					\end{tabular}
				\end{center}
					Figure \ref{fig:qlearningcartpole} include the episode vs reward plot for best three hyper-parameter combination.
					\begin{figure}[H]
						\centering
						\begin{subfigure}{.7\textwidth}
							\centering
							\includegraphics[width=\textwidth]{../cartpole-v1/plots/qlearning_Figure_1.png}
							\caption{$1$ vs $2$}
							\label{fig:qlearningcartpole1vs2}
						\end{subfigure}
						\hfill
						\begin{subfigure}{.7\textwidth}
							\centering
							\includegraphics[width=\textwidth]{../cartpole-v1/plots/qlearning_Figure_2.png}
							\caption{$1$ vs $3$}
							\label{fig:qlearningcartpole1vs3}
						\end{subfigure}
						\hfill
						\begin{subfigure}{.7\textwidth}
							\centering
							\includegraphics[width=\textwidth]{../cartpole-v1/plots/qlearning_Figure_3.png}
							\caption{$2$ vs $3$}
							\label{fig:qlearningcartpole2vs3}
						\end{subfigure}
						\caption{Performance comparison of the three best Q-Learning parameter configurations: (a) 1 vs 2, (b) 1 vs 3, and (c) 2 vs 3.}
						\label{fig:qlearningcartpole}
					\end{figure}
				
			\subsubsection{Result(SARSA vs Q-Learning)}
				The SARSA implementation achieved the highest average reward of 130 over 4000 episodes, while the Q-Learning implementation achieved an average reward of 119 over the same number of episodes.\\				
				From the graphs, we observe the following trends:
				\begin{itemize}
					\item SARSA: It takes longer to converge but ultimately achieves a higher average reward. The slower convergence indicates that SARSA explores more thoroughly, leading to better long-term performance.
					\item 	Q-Learning: Although Q-Learning converges faster, it tends to saturate at a lower reward, especially when different hyperparameters are used. This faster convergence comes at the cost of reduced average performance compared to SARSA.
				\end{itemize}
				In summary, SARSA demonstrates better stability and higher rewards in the long run, while Q-Learning converges more quickly but results in a slightly lower average reward.
		\subsection{\href{https://github.com/RitabrataMandal/RL-DA6400-assignment_1/tree/main/mountain_car-v0}{\textcolor{magenta}{MountainCar-v0}}}
			\subsubsection{Code Snippets}
				\lstinputlisting[language=Python,firstline=33, lastline=33, caption=Q-table ]{../mountain_car-v0/q_learning_softmax.py}
				\lstinputlisting[language=Python,firstline=22, lastline=30, caption=Discretized sates ]{../mountain_car-v0/sarsa.py}
				\lstinputlisting[language=Python,firstline=31, lastline=37, caption=epsilon-greedy action selection]{../mountain_car-v0/sarsa.py}
				\lstinputlisting[language=Python,firstline=40, lastline=78, caption= SARSA implementation ]{../mountain_car-v0/sarsa.py}
				\lstinputlisting[language=Python,firstline=31, lastline=41, caption=Softmax action selection ]{../mountain_car-v0/q_learning.py}
				\lstinputlisting[language=Python,firstline=45, lastline=80, caption=Q-Learning implementation]{../mountain_car-v0/q_learning.py}
			\subsubsection{SARSA  hyper-parameter tuning}
				Using Weights \& Biases (wandb) with a sweep method based on Bayesian optimization, we identified the best-performing hyper-parameters. Specifically, we set $\alpha \in [0.1, 0.5]$ and $\epsilon \in [0.01, 0.15]$, and ran 2000 episodes while minimizing the regret, defined as \(-200 -\) (all-time average return). See the wandb report on this environment \href{https://api.wandb.ai/links/ee24e009-iitm/4f545bce}{here}. Additionally, Figure \ref{fig:sarsamountaincar-regret} displays the results from 50 sweeps, illustrating the relationship between $\alpha$, $\epsilon$, and the regret.
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\linewidth]{sarsa-hyp-tuning-mountaincar.png}
					\caption{SARSA hyper-parameter sweeps for $\alpha$ and $\epsilon$ in MountainCar-v0(2000 episodes), with lines color-coded by regret.}
					\label{fig:sarsamountaincar-regret}
				\end{figure}
			\subsubsection{SARSA best $3$ results}
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|}
						\hline
						 & $\alpha$ & $\epsilon$ & $\gamma$ & regret\\
						\hline
						1 & 0.44517 & 0.01104 & 0.99 & 55.5099\\
						\hline
						2 & 0.44469 & 0.010567 & 0.99 & 55.6604 \\
						\hline
						3 & 0.36145 & 0.011928 & 0.99 & 56.547\\
						\hline
					\end{tabular}
				\end{center}
				Figure \ref{fig:sarsamountaincar} include the episode vs reward plot for best three hyper-parameter combination.
				\begin{figure}[H]
					\centering
					\begin{subfigure}{.8\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../mountain_car-v0/plots/sarsa_Figure_1.png}
						\caption{$1$ vs $2$}
						\label{fig:sarsamountaincar1vs2}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.8\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../mountain_car-v0/plots/sarsa_Figure_2.png}
						\caption{$1$ vs $3$}
						\label{fig:sarsamountaincar1vs3}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.8\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../mountain_car-v0/plots/sarsa_Figure_3.png}
						\caption{$2$ vs $3$}
						\label{fig:sarsamountaincar2vs3}
					\end{subfigure}
					\caption{Performance comparison of the three best SARSA parameter configurations: (a) 1 vs 2, (b) 1 vs 3, and (c) 2 vs 3.}
					\label{fig:sarsamountaincar}
				\end{figure}
			\subsubsection{Q-Learning hyper-parameter tuning}
				Using Weights \& Biases (wandb) with a sweep method based on Bayesian optimization, we identified the best-performing hyper-parameters. Specifically, we set $\alpha \in [0.1, 0.5]$ and $\tau \in [0.1, 1.5]$, and ran 2000 episodes while minimizing the regret, defined as \(-200 -\) (all-time average return). See the wandb report on this environment \href{https://wandb.ai/ee24e009-iitm/qlearning_mountain_car-v0tuning/reports/Q-Learning-MountainCar-v0---VmlldzoxMjAxNzEwMQ?accessToken=3ho3ppomqsu6prg4yd4wdhset84tq72p24zrpw8ztwcivdw8xfsjevl6tduvjfi5}{here}. Additionally, Figure \ref{fig:qlearningmountaincar-regret} displays the results from 50 sweeps, illustrating the relationship between $\alpha$, $\tau$, and the regret.
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\linewidth]{qlearning-hyp-tuning-mountaincar.png}
					\caption{Q-Learning hyper-parameter sweeps for $\alpha$ and $\tau$ in MountainCar-v0(2000 episodes), with lines color-coded by regret.}
					\label{fig:qlearningmountaincar-regret}
				\end{figure}
			\subsubsection{Q-Learning best $3$ results}
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|}
						\hline
						& $\alpha$ & $\tau$ & $\gamma$ & regret\\
						\hline
						1 & 0.49061 & 0.14082 & 0.99 & 59.6092\\
						\hline
						2 & 0.47706 & 0.10075 & 0.99 &60.0469\\
						\hline
						3 & 0.49608 & 0.11588 & 0.99 &60.4235\\
						\hline
					\end{tabular}
				\end{center}
				Figure \ref{fig:qlearningmountaincar} include the episode vs reward plot for best three hyper-parameter combination.
				\begin{figure}[H]
					\centering
					\begin{subfigure}{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../mountain_car-v0/plots/qlearning_Figure_1.png}
						\caption{$1$ vs $2$}
						\label{fig:qlearningmountaincar1vs2}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.8\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../mountain_car-v0/plots/qlearning_Figure_2.png}
						\caption{$1$ vs $3$}
						\label{fig:qlearningmountaincar1vs3}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.8\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../mountain_car-v0/plots/qlearning_Figure_3.png}
						\caption{$2$ vs $3$}
						\label{fig:qlearningmountaincar2vs3}
					\end{subfigure}
					\caption{Performance comparison of the three best Q-Learning parameter configurations: (a) 1 vs 2, (b) 1 vs 3, and (c) 2 vs 3.}
					\label{fig:qlearningmountaincar}
				\end{figure}
			\subsubsection{Result(SARSA vs Q-Learning)}		
				The SARSA implementation achieved the best average reward of -255 over 2000 episodes, while the Q-Learning implementation achieved an average reward of -259 over the same period.\\				
				From these results, we can conclude the following:
				\begin{itemize}
					\item SARSA with $\epsilon$-greedy: It performs slightly better, achieving a marginally higher average reward.
					\item Q-Learning: Although it converges to a similar solution, its average reward remains slightly lower than that of SARSA.
				\end{itemize}				
				In summary, both SARSA with $\epsilon$-greedy and Q-Learning successfully solve the environment, producing comparable results with only a minor difference in performance.
		\subsection{\href{https://github.com/RitabrataMandal/RL-DA6400-assignment_1/tree/main/minigrid_world}{\textcolor{magenta}{MiniGrid-Dynamic-Obstacles-5x5-v0}}}
			\subsubsection{Code Snippets}
				Action selection using $\epsilon$-greedy
				\lstinputlisting[language=Python,firstline=7, lastline=11, caption=epsilon-greedy action selection]{../minigrid_world/sarsa.py}
				\lstinputlisting[language=Python,firstline=16, lastline=56, caption= SARSA implementation ]{../minigrid_world/sarsa.py}
				\lstinputlisting[language=Python,firstline=6, lastline=10, caption=Softmax action selection ]{../minigrid_world/q_learning.py}
				\lstinputlisting[language=Python,firstline=14, lastline=55, caption=Q-Learning implementation]{../minigrid_world/q_learning.py}
			\subsubsection{SARSA hyper-parameter tuning}
			Using Weights \& Biases (wandb) with a sweep method based on Bayesian optimization, we identified the best-performing hyper-parameters. Specifically, we set $\alpha \in [0.1, 0.5]$ and $\epsilon \in [0.01, 0.15]$, and ran 2000 episodes while minimizing the regret, defined as \(1 -\)(all-time average return). See the wandb report on this environment \href{https://api.wandb.ai/links/ee24e009-iitm/axvqh3jk}{here}. Additionally, Figure \ref{fig:sarsaminigridworld-regret} displays the results from 50 sweeps, illustrating the relationship between $\alpha$, $\epsilon$, and the regret.
			\begin{figure}[H]
				\centering
				\includegraphics[width=1\linewidth]{sarsa-hyp-tuning-minigridworld.png}
				\caption{SARSA hyper-parameter sweeps for $\alpha$ and $\epsilon$ in MiniGrid-Dynamic-Obstacles-5x5-v0(2000 episodes), with lines color-coded by regret.}
				\label{fig:sarsaminigridworld-regret}
			\end{figure}
			\subsubsection{SARSA best $3$ results}
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|}
						\hline
						& $\alpha$ & $\epsilon$ &$\gamma$& regret\\
						\hline
						1 & 0.12704 & 0.02681 & 0.99 & 0.19653\\
						\hline
						2 & 0.2107 & 0.024791 & 0.99 & 0.19896\\
						\hline
						3 & 0.12801 & 0.022713 & 0.99 & 0.199997\\
						\hline
					\end{tabular}
				\end{center}
				Figure \ref{fig:sarsaminigridworld} include the episode vs reward plot for best three hyper-parameter combination.
				\begin{figure}[H]
					\centering
					\begin{subfigure}{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../minigrid_world/plots/sarsa_Figure_1.png}
						\caption{$1$ vs $2$}
						\label{fig:sarsaminigridworld1vs2}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../minigrid_world/plots/sarsa_Figure_2.png}
						\caption{$1$ vs $3$}
						\label{fig:sarsaminigridworld1vs3}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../minigrid_world/plots/sarsa_Figure_3.png}
						\caption{$2$ vs $3$}
						\label{fig:sarsaminigridworld2vs3}
					\end{subfigure}
					\caption{Performance comparison of the three best SARSA parameter configurations: (a) 1 vs 2, (b) 1 vs 3, and (c) 2 vs 3.}
					\label{fig:sarsaminigridworld}
				\end{figure}
			\subsubsection{Q-Learning hyper-parameter tuning}
			Using Weights \& Biases (wandb) with a sweep method based on Bayesian optimization, we identified the best-performing hyper-parameters. Specifically, we set $\alpha \in [0.1, 0.5]$ and $\tau \in [0.1, 1.5]$, and ran 2000 episodes while minimizing the regret, defined as \(1 -\) (all-time average return). See the wandb report on this environment \href{https://wandb.ai/ee24e009-iitm/minigrid-qlearning-softmax-tuning/reports/Q-Learning-MiniGrid-5x5-v0---VmlldzoxMjAyMTY4OQ?accessToken=7k22frexodx9vs7tryakvnme6ubxd011hcul9mke18nlhsfc8jtpo73poi45toq8}{here}. Additionally, Figure \ref{fig:qlearningminigridworld-regret} displays the results from 50 sweeps, illustrating the relationship between $\alpha$, $\tau$, and the regret.
			\begin{figure}[H]
				\centering
				\includegraphics[width=1\linewidth]{qlearning-hyp-tuning-minigridworld.png}
				\caption{Q-Learning hyper-parameter sweeps for $\alpha$ and $\tau$ in MiniGrid-Dynamic-Obstacles-5x5-v0(2000 episodes), with lines color-coded by regret.}
				\label{fig:qlearningminigridworld-regret}
			\end{figure}
			\subsubsection{Q-Learning best $3$ results}
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|}
						\hline
						& $\alpha$ & $\tau$ & $\gamma$ & regret\\
						\hline
						1 & 0.10866 & 0.50035 & 0.99 & 1.22505\\
						\hline
						2 & 0.11349 & 0.5035 & 0.99 & 1.25063\\
						\hline
						3 & 0.10321 & 0.50503 & 0.99 & 1.25763\\
						\hline
					\end{tabular}
				\end{center}
				Figure \ref{fig:qlearningminigridworld} include the episode vs reward plot for best three hyper-parameter combination.
				\begin{figure}[H]
					\centering
					\begin{subfigure}{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../minigrid_world/plots/qlearning_Figure_1.png}
						\caption{$1$ vs $2$}
						\label{fig:qlearningminigridworld1vs2}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../minigrid_world/plots/qlearning_Figure_2.png}
						\caption{$1$ vs $3$}
						\label{fig:qlearningminigridworld1vs3}
					\end{subfigure}
					\hfill
					\begin{subfigure}{.7\textwidth}
						\centering
						\includegraphics[width=\textwidth]{../minigrid_world/plots/qlearning_Figure_3.png}
						\caption{$2$ vs $3$}
						\label{fig:qlearningminigridworld2vs3}
					\end{subfigure}
					\caption{Performance comparison of the three best Q-Learning parameter configurations: (a) 1 vs 2, (b) 1 vs 3, and (c) 2 vs 3.}
					\label{fig:qlearningminigridworld}
				\end{figure}
			\subsubsection{Result(SARSA vs Q-Learning)}
			The SARSA implementation achieved a best average reward of \(0.8\) over 2000 episodes, whereas Q-learning achieved \(-0.2\) over the same number of episodes. Hence SARSA with $\epsilon$-greedy solves the environment better than Q-Learning with softmax exploration strategy.
	\section{Github link}	
		 \href{https://github.com/RitabrataMandal/RL-DA6400-assignment_1}{\text{https://github.com/RitabrataMandal/RL-DA6400-assignment\_1}}		
		\section{Requirements and Hyperparameter Configurations}
		
			\subsection{Requirements}
				The required packages and dependencies are listed in the \texttt{requirements.txt} file:
				
				\VerbatimInput{../requirements.txt}
			
			\subsection{Hyperparameter Configurations}
				The hyperparameter configurations for SARSA and Q-Learning are provided in the following YAML files:
				
				\begin{itemize}
					\item \textbf{SARSA Configuration:} The configuration for SARSA is defined in the file \texttt{sweep\_sarsa.yaml}.
					\lstinputlisting[language=yaml, style=mystyle, caption={\texttt{sweep\_sarsa.yaml}}]{../minigrid_world/sweep_sarsa.yaml}
					
					\item \textbf{Q-Learning Configuration:} The configuration for Q-Learning is defined in the file \texttt{sweep\_qlearning.yaml}.
					\lstinputlisting[language=yaml, style=mystyle, caption={\texttt{sweep\_qlearning.yaml}}]{../minigrid_world/sweep_config.yaml}
				\end{itemize}
	\section{Steps to Recreate}
	To set up and run the project, follow these steps:
	
	\begin{enumerate}
		\item \textbf{Install Required Packages:}
		\begin{itemize}
			\item Run the following command to install all necessary packages:
			\begin{verbatim}
				pip install -r requirements.txt
			\end{verbatim}
		\end{itemize}
		
		\item \textbf{Run Experiments for Different Environments:}
		\begin{enumerate}
			\item \textbf{CartPole-v1:}
			\begin{itemize}
				\item Navigate to the \texttt{cartpole-v1} directory:
				\begin{verbatim}
					cd cartpole-v1
				\end{verbatim}
				\item Run the appropriate script:
				\begin{itemize}
					\item For SARSA implementation:
					\begin{verbatim}
						python sarsa.py
					\end{verbatim}
					\item For Q-Learning implementation:
					\begin{verbatim}
						python qlearning.py
					\end{verbatim}
				\end{itemize}
			\end{itemize}
			
			\item \textbf{Mountain\_Car-v0:}
			\begin{itemize}
				\item Navigate to the \texttt{mountain\_car-v0} directory:
				\begin{verbatim}
					cd mountain_car-v0
				\end{verbatim}
				\item Run the appropriate script:
				\begin{itemize}
					\item For SARSA implementation:
					\begin{verbatim}
						python sarsa.py
					\end{verbatim}
					\item For Q-Learning implementation:
					\begin{verbatim}
						python q_learning.py
					\end{verbatim}
				\end{itemize}
			\end{itemize}
			
			\item \textbf{MiniGrid-Dynamic-Obstacles-5x5-v0:}
			\begin{itemize}
				\item Navigate to the \texttt{minigrid\_world} directory:
				\begin{verbatim}
					cd minigrid_world
				\end{verbatim}
				\item Run the appropriate script:
				\begin{itemize}
					\item For SARSA implementation:
					\begin{verbatim}
						python sarsa_epsilon_greedy.py
					\end{verbatim}
					\item For Q-Learning implementation:
					\begin{verbatim}
						python q_learning_softmax.py
					\end{verbatim}
				\end{itemize}
			\end{itemize}
		\end{enumerate}
	\end{enumerate}
		
\end{document}